{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OTeZWA9VgT-k"
   },
   "outputs": [],
   "source": [
    "#  Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "#  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#  you may not use this file except in compliance with the License.\n",
    "#  You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#  Unless required by applicable law or agreed to in writing, software\n",
    "#  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#  See the License for the specific language governing permissions and\n",
    "#  limitations under the License.\n",
    "\"\"\"An Example of a DNNClassifier for the Iris dataset.\"\"\"\n",
    "\n",
    "# https://www.tensorflow.org/get_started/get_started_for_beginners\n",
    "# J'ai modifié l'exemple Getting Started for ML Beginners\n",
    "# J'ai supprimé le import iris_data et j'ai intégré ici les fonctions définies dans le fichier iris_data\n",
    "# Je commente à la fois le Python et le TensorFlow\n",
    "\n",
    "# https://docs.python.org/2/library/__future__.html\n",
    "# __future__ is a real module, and serves three purposes:\n",
    "# To avoid confusing existing tools that analyze import statements and expect to find the modules they’re importing.\n",
    "# To ensure that future statements run under releases prior to 2.1 at least yield runtime exceptions \n",
    "# (the import of __future__ will fail, because there was no module of that name prior to 2.1).\n",
    "# To document when incompatible changes were introduced, and when they will be — or were — made mandatory. \n",
    "# This is a form of executable documentation, and can be inspected programmatically via \n",
    "# importing __future__ and examining its contents.\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# https://docs.python.org/3/library/argparse.html#argumentparser-objects\n",
    "# parser = argparse.ArgumentParser()\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MBwI_hqMiQ30"
   },
   "outputs": [],
   "source": [
    "# https://docs.python.org/3/library/argparse.html#argparse.ArgumentParser.add_argument\n",
    "# parser.add_argument('--batch_size', default=100, type=int, help='batch size')\n",
    "# parser.add_argument('--train_steps', default=1000, type=int, help='number of training steps')\n",
    "# J'ai remplacé le argv par des constantes\n",
    "\n",
    "# Test set accuracy: 0.967 si je mets CONST_BATCH_SIZE = 100\n",
    "# Test set accuracy: 0.967 si je mets CONST_BATCH_SIZE = 1000\n",
    "# CONST_BATCH_SIZE = 1000 par défaut\n",
    "CONST_BATCH_SIZE = 1000\n",
    "\n",
    "# Test set accuracy: 0.533 si je mets CONST_BATCH_SIZE = 1000 et CONST_TRAINING_STEPS = 10\n",
    "# Test set accuracy: 0.533 si je mets CONST_BATCH_SIZE = 1000 et CONST_TRAINING_STEPS = 200\n",
    "# Si je mets CONST_TRAINING_STEPS = 50 alors Test set accuracy: 0.833\n",
    "\n",
    "# CONST_TRAINING_STEPS = 100 par défaut\n",
    "CONST_TRAINING_STEPS = 100\n",
    "\n",
    "\n",
    "TRAIN_URL = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "# 120,4,setosa,versicolor,virginica\n",
    "# 6.4,2.8,5.6,2.2,2\n",
    "# 5.0,2.3,3.3,1.0,1\n",
    "# 4.9,2.5,4.5,1.7,2\n",
    "# ...\n",
    "TEST_URL = \"http://download.tensorflow.org/data/iris_test.csv\"\n",
    "# 30,4,setosa,versicolor,virginica\n",
    "# 5.9,3.0,4.2,1.5,1\n",
    "# 6.9,3.1,5.4,2.1,2\n",
    "# 5.1,3.3,1.7,0.5,0\n",
    "# 6.0,3.4,4.5,1.6,1\n",
    "\n",
    "# 0 represents setosa\n",
    "# 1 represents versicolor\n",
    "# 2 represents virginica\n",
    "\n",
    "CSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth',\n",
    "                    'PetalLength', 'PetalWidth', 'Species']\n",
    "SPECIES = ['Setosa', 'Versicolor', 'Virginica']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nYKpD8P2iVKO"
   },
   "outputs": [],
   "source": [
    "def maybe_download():\n",
    "    \n",
    "    # https://www.tensorflow.org/api_docs/python/tf/keras/utils/get_file\n",
    "    # Downloads a file from a URL if it not already in the cache.\n",
    "\n",
    "    # By default the file at the url origin is downloaded to the cache_dir ~/.keras, placed in \n",
    "    # the cache_subdir datasets, and given the filename fname. \n",
    "    # The final location of a file example.txt would therefore be ~/.keras/datasets/example.txt.\n",
    "    # get_file(\n",
    "    #   fname,\n",
    "    #   origin,\n",
    "    # ...\n",
    "    # )\n",
    "    # On fait d'abord un split sur /\n",
    "    # TEST_URL = \"http://download.tensorflow.org/data/iris_test.csv\"\n",
    "    # ['http:', '', 'download.tensorflow.org', 'data', 'iris_training.csv']\n",
    "    # Puis [-1] pour obtenir le dernier élément de la liste, soit iris_training.csv ici\n",
    "    # Le résultat obtenu pour train_path est sur mon Mac : /Users/xxx/.keras/datasets/iris_training.csv\n",
    "    train_path = tf.keras.utils.get_file(TRAIN_URL.split('/')[-1], TRAIN_URL)\n",
    "    test_path = tf.keras.utils.get_file(TEST_URL.split('/')[-1], TEST_URL)\n",
    "\n",
    "    return train_path, test_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eJzvrE6iiYcb"
   },
   "outputs": [],
   "source": [
    "def load_data(y_name='Species'):\n",
    "    \"\"\"Returns the iris dataset as (train_x, train_y), (test_x, test_y).\"\"\"\n",
    "    train_path, test_path = maybe_download()\n",
    "\n",
    "    train = pd.read_csv(train_path, names=CSV_COLUMN_NAMES, header=0)\n",
    "    # train : \n",
    "    #\n",
    "    #     SepalLength  SepalWidth  PetalLength  PetalWidth\n",
    "    # 0            6.4         2.8          5.6         2.2\n",
    "    # 1            5.0         2.3          3.3         1.0\n",
    "    # 2            4.9         2.5          4.5         1.7\n",
    "    # 3            4.9         3.1          1.5         0.1\n",
    "    # ...\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.pop.html\n",
    "    # Return item and drop from frame. \n",
    "    train_x, train_y = train, train.pop(y_name)\n",
    "\n",
    "    test = pd.read_csv(test_path, names=CSV_COLUMN_NAMES, header=0)\n",
    "    \n",
    "    # https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.pop.html\n",
    "    # Return item and drop from frame. \n",
    "    test_x, test_y = test, test.pop(y_name)\n",
    "\n",
    "    return (train_x, train_y), (test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "78ygzRfmij77"
   },
   "outputs": [],
   "source": [
    "def train_input_fn(features, labels, batch_size):\n",
    "    \"\"\"An input function for training\"\"\"\n",
    "    # Convert the inputs to a Dataset.\n",
    "    # Creates a Dataset whose elements are slices of the given tensors.\n",
    "    # https://www.tensorflow.org/versions/r1.2/api_docs/python/tf/contrib/data/Dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "\n",
    "    # Shuffle, repeat, and batch the examples.\n",
    "    # Randomly shuffles the elements of this dataset.\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
    "    # buffer_size: A tf.int64 scalar tf.Tensor, representing the number of elements from this dataset\n",
    "    # from which the new dataset will sample.\n",
    "    dataset = dataset.shuffle(CONST_BATCH_SIZE).repeat().batch(batch_size)\n",
    "\n",
    "    # Build the Iterator, and return the read end of the pipeline.\n",
    "    # https://www.tensorflow.org/programmers_guide/datasets#creating_an_iterator\n",
    "    # A one-shot iterator is the simplest form of iterator, which only supports iterating \n",
    "    # once through a dataset, with no need for explicit initialization. One-shot iterators handle \n",
    "    # almost all of the cases that the existing queue-based input pipelines support, \n",
    "    # but they do not support parameterization.\n",
    "    \n",
    "    # Creates an Iterator for enumerating the elements of this dataset.\n",
    "    # https://www.tensorflow.org/versions/r1.2/api_docs/python/tf/contrib/data/Dataset\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/data/Dataset\n",
    "    # Creates an Iterator for enumerating the elements of this dataset.\n",
    "    return dataset.make_one_shot_iterator().get_next()\n",
    "    # return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cwUnNK8yim5V"
   },
   "outputs": [],
   "source": [
    "def eval_input_fn(features, labels, batch_size):\n",
    "    \"\"\"An input function for evaluation or prediction\"\"\"\n",
    "    features=dict(features)\n",
    "    \n",
    "    if labels is None:\n",
    "        # No labels, use only features.\n",
    "        inputs = features\n",
    "    else:\n",
    "        inputs = (features, labels)\n",
    "    \n",
    "\n",
    "    # Convert the inputs to a Dataset.\n",
    "    # dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
    "    # dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
    "\n",
    "    # Batch the examples\n",
    "    assert batch_size is not None, \"batch_size must not be None\"\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    # Return the dataset.\n",
    "    # return dataset\n",
    "    return dataset.make_one_shot_iterator().get_next()\n",
    "    # return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "id": "uDEZIp98yOjm",
    "outputId": "b98e1513-e0d3-4201-9e0b-eab291e6f413"
   },
   "outputs": [],
   "source": [
    "# On peut aussi lire les données avec sklearn mais je ne vais pas le faire car \n",
    "# je veux être sur que le training set et le test set restent composés comme \n",
    "# dans l'exercice\n",
    "\n",
    "# from sklearn.datasets import load_iris\n",
    "# data = load_iris()\n",
    "# data.DESCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r-5BQwhg3z2e"
   },
   "outputs": [],
   "source": [
    "# ---------------------- C'est ici que ça commence ----------------------\n",
    "#\n",
    "#\n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "# Je ne trouve pas de doc assez claire sur le tf.logging.INFO\n",
    "# tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "(train_x, train_y), (test_x, test_y) = load_data() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "mc01oIsS32xb",
    "outputId": "0fd004bc-7023-475e-f893-a65c9528c644"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SepalLength</th>\n",
       "      <th>SepalWidth</th>\n",
       "      <th>PetalLength</th>\n",
       "      <th>PetalWidth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.9</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SepalLength  SepalWidth  PetalLength  PetalWidth\n",
       "0          6.4         2.8          5.6         2.2\n",
       "1          5.0         2.3          3.3         1.0\n",
       "2          4.9         2.5          4.5         1.7\n",
       "3          4.9         3.1          1.5         0.1\n",
       "4          5.7         3.8          1.7         0.3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "YElI69fF3-Zc",
    "outputId": "5bed4906-48af-43ac-d815-65d1cb8f317a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2\n",
       "1    1\n",
       "2    2\n",
       "3    0\n",
       "4    0\n",
       "Name: Species, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "PBzEHDMB4HWh",
    "outputId": "45721088-7977-42d7-e733-cf8dc611ddc7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SepalLength</th>\n",
       "      <th>SepalWidth</th>\n",
       "      <th>PetalLength</th>\n",
       "      <th>PetalWidth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SepalLength  SepalWidth  PetalLength  PetalWidth\n",
       "0          5.9         3.0          4.2         1.5\n",
       "1          6.9         3.1          5.4         2.1\n",
       "2          5.1         3.3          1.7         0.5\n",
       "3          6.0         3.4          4.5         1.6\n",
       "4          5.5         2.5          4.0         1.3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "8EqMmVsD4Ojq",
    "outputId": "0c05b4e8-2b38-4546-de31-b64e19a2fbbb",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    2\n",
       "2    0\n",
       "3    1\n",
       "4    1\n",
       "Name: Species, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yp7YJizK4Y5I"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Feature columns describe how to use the input.\n",
    "# A feature column is a data structure that tells your model how to interpret the data \n",
    "# in each feature. In the Iris problem, we want the model to interpret the data in each feature \n",
    "# as its literal floating-point value; that is, we want the model to interpret an input value \n",
    "# like 5.4 as, well, 5.4. However, in other machine learning problems, it is often desirable to interpret data less literally.\n",
    "\n",
    "my_feature_columns = []\n",
    "\n",
    "# my_feature_columns \n",
    "# \t[\n",
    "# \t\t_NumericColumn(key='SepalLength', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), \n",
    "# \t\t_NumericColumn(key='SepalWidth', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), \n",
    "# \t\t_NumericColumn(key='PetalLength', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), \n",
    "# \t\t_NumericColumn(key='PetalWidth', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)\n",
    "# \t]\n",
    "for key in train_x.keys():\n",
    "    my_feature_columns.append(tf.feature_column.numeric_column(key=key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "CxUg5Iuv4b_H",
    "outputId": "dc677383-f8ea-42d4-8818-b8264b6f9f4a",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NumericColumn(key='SepalLength', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='SepalWidth', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='PetalLength', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None),\n",
       " NumericColumn(key='PetalWidth', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 503
    },
    "colab_type": "code",
    "id": "P5teZyY74ogS",
    "outputId": "904f378d-9e21-459e-e949-4fe0da4e2f59"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0623 01:15:41.900930 139955597150016 estimator.py:1811] Using temporary folder as model directory: /tmp/tmpucmatbj_\n",
      "W0623 01:15:41.926897 139955597150016 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "W0623 01:15:41.968855 139955597150016 deprecation.py:323] From <ipython-input-5-224d5c8a6ac8>:26: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
      "W0623 01:15:42.015345 139955597150016 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0623 01:15:43.086353 139955597150016 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/canned/head.py:437: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W0623 01:15:43.235462 139955597150016 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0623 01:15:43.364973 139955597150016 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.canned.dnn.DNNClassifier at 0x7f49af850cf8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build 2 hidden layer DNN with 10, 10 units respectively.\n",
    "# To implement a neural network, the premade_estimators.py program uses a pre-made Estimator \n",
    "# named tf.estimator.DNNClassifier. This Estimator builds a neural network that classifies examples. \n",
    "# Nous avons un Réseau de Neurons avec 3 couches \n",
    "# Chaque couche a 10 neurons\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=my_feature_columns,\n",
    "    # Two hidden layers of 10 nodes each.\n",
    "    # Use the hidden_units parameter to define the number of neurons in each hidden layer of the neural network. \n",
    "    # The length of the list assigned to hidden_units identifies the number of hidden layers (2, in this case). \n",
    "    # Each value in the list represents the number of neurons in a particular hidden \n",
    "    # layer (10 in the first hidden layer and 10 in the second hidden layer). \n",
    "    # To change the number of hidden layers or neurons, simply assign a different list to the hidden_units parameter.\n",
    "    \n",
    "    hidden_units=[10, 10],\n",
    "    # The model must choose between 3 classes.\n",
    "    n_classes=3)\n",
    "\n",
    "# Train the Model.\n",
    "# classifier.train(input_fn=lambda:iris_data.train_input_fn(train_x, train_y,100),steps=1000)\n",
    "classifier.train(input_fn=lambda:train_input_fn(train_x, train_y, CONST_BATCH_SIZE), steps=CONST_TRAINING_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "colab_type": "code",
    "id": "sJlBpzabipYd",
    "outputId": "3c5c31b0-a694-4028-9193-a76b83ef1e9f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0623 01:16:35.220588 139955597150016 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set accuracy: 1.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model.\n",
    "eval_result = classifier.evaluate(input_fn=lambda:eval_input_fn(test_x, test_y, CONST_BATCH_SIZE))\n",
    "print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kRktbQbwiveI"
   },
   "outputs": [],
   "source": [
    "# Generate predictions from the model\n",
    "expected = ['Setosa', 'Versicolor', 'Virginica']\n",
    "\n",
    "predict_x = {\n",
    "    'SepalLength': [5.1, 5.9, 6.9],\n",
    "    'SepalWidth': [3.3, 3.0, 3.1],\n",
    "    'PetalLength': [1.7, 4.2, 5.4],\n",
    "    'PetalWidth': [0.5, 1.5, 2.1],\n",
    "}\n",
    "\n",
    "# \tSepalLength\t\tSepalWidth\tPetalLength\t\tPetalWidth\n",
    "# \t\t5.1\t\t\t3.3\t\t\t\t1.7\t\t\t\t0.5\n",
    "# \t\t5.9\t\t\t3.0\t\t\t\t4.2\t\t\t\t1.5\n",
    "# \t\t6.9\t\t\t3.1\t\t\t\t5.4\t\t\t\t2.1\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier\n",
    "predictions = classifier.predict(input_fn=lambda:eval_input_fn(predict_x,labels=None,batch_size=CONST_BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qj4dhlHfiGs9"
   },
   "outputs": [],
   "source": [
    "# zip\n",
    "# This function returns a list of tuples, where the i-th tuple contains the i-th element from each of \n",
    "# the argument sequences or iterables. The returned list is truncated in length to the length of the \n",
    "# shortest argument sequence. When there are multiple arguments which are all of the same length, \n",
    "# zip() is similar to map() with an initial argument of None. \n",
    "# With a single sequence argument, it returns a list of 1-tuples. With no arguments, it returns an empty list.\n",
    "\n",
    "# >>> x = [1, 2, 3]\n",
    "# >>> y = [4, 5, 6]\n",
    "# >>> zipped = zip(x, y)\n",
    "# >>> zipped\n",
    "# [(1, 4), (2, 5), (3, 6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "zXxQsQ-hhiFs",
    "outputId": "27c8ac6a-8d39-424b-fd6a-e891bb36bb0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction is \"Setosa\" (94.9%), expected \"Setosa\"\n",
      "\n",
      "Prediction is \"Versicolor\" (77.7%), expected \"Versicolor\"\n",
      "\n",
      "Prediction is \"Virginica\" (62.7%), expected \"Virginica\"\n"
     ]
    }
   ],
   "source": [
    "for pred_dict, expec in zip(predictions, expected):\n",
    "    template = ('\\nPrediction is \"{}\" ({:.1f}%), expected \"{}\"')\n",
    "    class_id = pred_dict['class_ids'][0]\n",
    "    probability = pred_dict['probabilities'][class_id]\n",
    "    print(template.format(SPECIES[class_id], 100 * probability, expec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "iris_Tf.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
